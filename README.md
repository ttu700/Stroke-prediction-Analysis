# Stroke-prediction-Analysis

## Presentation
### [R Strokes Analysis Presentation](https://github.com/ttu700/Stroke-prediction-Analysis/tree/main/Presentation/Final%20Presentation.pdf)


## Table Of Contents
[Summary of Data Manipulations and Feature Engineering](https://github.com/ttu700/Stroke-prediction-Analysis#summary-of-data-manipulations-and-feature-engineering)

[Summary of Statistical Methods Used](https://github.com/ttu700/Stroke-prediction-Analysis#summary-of-statistical-methods-used)

[Key Observations and Results from Statistical Models](https://github.com/ttu700/Stroke-prediction-Analysis#key-observations-and-results-from-statistical-models)

[Conclusions from the Analysis](https://github.com/ttu700/Stroke-prediction-Analysis#conclusions-from-the-analysis)

[Lessons Learned in the Project](https://github.com/ttu700/Stroke-prediction-Analysis#lessons-learned-in-the-project)

[Data Source and References](https://github.com/ttu700/Stroke-prediction-Analysis#data-source-and-references)

## Summary of Data Manipulations and Feature Engineering
1. **Converted 'Gender' to binary variable (0 for male, 1 for female)**
2. **Converted 'Ever_married' and 'Residence_Type' to numeric binary units**
3. **Removed 'id' variable as it had no real value**
4. **Removed "NA" variables skewing data, particularly affecting 'BMI'**
5. **Converted variables with multiple levels into factors**
6. **Used 'as.factor' on 'work type' and 'smoking status'**
7. **Made 'stroke' variable a factor for decision trees**
8. **Pruned decision trees to decrease variance**

In order to better be able to use the dataset, we had to make sure to modify it, be it with things like data type (character, integer, numeric, etc.), types of data we wanted to include, and the amount of data we would be taking into consideration (for our forecasting/predictive models). In the beginning, we made sure to make the data readable for linear regression; we did not end up using simple linear regression as our data had a binary response variable, but this change was still initially important. ‘Gender’ was converted to a binary variable with 0 if patients identified as male and 1 if they identified as female while ‘Ever_married’ and ‘Residence_Type’ had both been changed to numeric binary units from character units with: 'Ever_married', 1 if Yes and 0 if No and 'Residence_type', 1 if Rural and 0 if Urban. Also, we made sure to remove the ‘id’ variable, or the personal identifier for the patients in the data set since it did not hold any real value to us. These were all changed with tweaking around the command of ‘as.factor’ and ‘as.numeric’ to make the data ready for modeling. This was done with the command of dat[,2:12] which included every variable besides ‘id’ into the data set we would use for modeling. Down the line we omitted all “NA” type variables as they were skewing the data away from the potentially highly correlated variable of ‘BMI’. Without this, ‘Ever_Married’ seemed like one of the better predictors of a stroke while in reality, BMI should be making a big difference in whether or not a certain individual would experience a stroke. After that, we made the variables that could potentially have multiple levels for more accurate modeling into factors. We used just the ‘as.factor’ command (without ‘as.numeric’ or without subtracting anything in order to make them binary) on ‘work type’ and ‘smoking status’. It would have been easier if our data set only had two responses for smoking status but the response of ‘unknown’ was also included, muddying up the ease of modeling. For decision trees, we made the stroke variable into a factor in order to allow for modeling and the initial split. We then prune multiple models in different ways. We remove many sets and observations throughout, which can be seen by the ‘rm’ command. Also, we prune our data tree in order to have the ‘best’ outcome we could, decreasing variance.

## Summary of Statistical Methods Used
1. **Partitioned data into test and training sets**
2. **Used logistic regression and classification trees for modeling**
3. **Tested logistic regression with 'glm' command and evaluated accuracy**
4. **Created and trained decision trees using 'tree' package**
5. **Pruned decision trees and ran cross-validation**
6. **Explored bagging and gradient boosting methods**
7. **Chose bagging with 10 trees as the best model**

Statistical methods used mainly consisted of model creation by means of classification trees and logistic regression. For both models, we decided to partition the data by creating two datasets: a test and a training set. This was accomplished by first choosing a random seed using the ‘set. seed’ command then using the ‘subset’ and ‘sample’ commands in order to create two sets each with a sample size of 105 equal to half of the observations that responded with ‘yes’ from the stroke data table. Rows for the test set were similarly created by using the ‘leftoverrows’ command and specifying 2350 or half of the observations that responded with ‘no’ from the stroke data table. In this way, we created a training set with an equal ratio and a test set with a ratio equal to the stroke data table. Once finished, we removed any unneeded data sets using the ‘rm’ command in order to prepare for model training and predictions. In order to create and train the logistic regression model, we used the ‘glm’ command adding ‘family = binomial’ and specified stroke compared to all other variables using the training set. The forecasting was done by using the ‘predict’ command on the log reg model and including ‘type = response’. We then took the first 20 predictions and compared them to the first 20 for the actual dataset. This gave us an idea of the accuracy of the predictor which we then proceeded to calculate using the ‘mean’ command on the test dataset and the values for stroke from the dataset. A confusion matrix was also constructed to better visualize the results using the ‘table’ command. Specific class errors were computed using the confusion matrix. As for the classification tree, we first changed the stroke variable to a factor then installed the tree package and used the ‘tree’ command to create and train a tree using the training data on the stroke variable. We then plotted the tree and proceeded to test it by making a prediction on the test dataset. In order to mitigate some of the high variance and attempt to reduce error, we  proceeded to prune the created tree using ‘prune. misclass’ and found that a tree with 2 nodes was the optimal node count and ran cross-validation using ‘cv.tree’ with K = 10  in order to see if there is a lower variance model. After determining that there were minor differences in error, we utilized various other methods such as bagging and gradient boosting. To create each model, we first installed the ‘randomForest’ package and used the ‘randomForest’ command on the training set with a size of 10 trees to create the bagging model. Comparisons were made using 10, 25, 100, and 1000 trees. We then ran a prediction and evaluated the mean for each size. In order to run gradient boosting, we changed the stroke variable back to quantitative. After installing the ‘gbm’ package, we were able to use the ‘gbm’ command while specifying distribution type as Bernoulli, number of trees, interaction depth, and shrinkage on the training set. This created a model which we then tested with the ‘predict’ command and evaluated using the ‘mean’ command to find the error. We found that bagging produced a lower error than the other method and used it to create predictions on the test data set. For our model prediction, we decided on the bagging method with 10 trees as it produced the best result when compared to bagging with higher numbers of trees. 

## Key Observations and Results from Statistical Models
1. **Only about 5% of the dataset experienced a stroke**
2. **'Ever_married' seemed correlated with stroke, but age-related factors played a role**
3. **Clear link between higher 'glucose' levels and stroke**
4. **Logistic regression showed 'age' as the only significant variable**
5. **Decision tree variables: female, age, hypertension, average glucose level, BMI, rural residence, smoking status**
6. **Recognized the difficulty of predicting strokes due to the complex nature of contributing factors**

In our more general, descriptive models, we were able to see that in a general population (if we generalize the responses from the data set) there aren’t that many events of a stroke happening - though this problem isn’t necessarily widespread, its effects can still be highly detrimental and so we should keep the variables that can cause it in mind. To be more technical, only about five percent of our total data set experienced a stroke in the past, though many are at risk due to our findings. One of the biggest shockers can be seen when we look at the ‘ever_married’ variable against our response variable of ‘stroke’. Though we can see that not many of the individuals in our data set are married when looking at how many of them have had a stroke, almost all of the people who had a stroke were married. However, we later realized that this may be misleading since marriage comes with a lot of other factors tied to it too, such as age, hypertension, and high blood pressure. Thus, it makes sense that the older one is, the more likely they are to get married, and thus this may skew the results to make it seem like marriage is a big contributor to the probability of having a stroke. Alternatively, when we look at the variable of ‘glucose’ there seems to be a very clear and rational pattern and link between higher glucose levels and having a stroke - the median level of those who had a stroke was much higher than those individuals who did not, which our boxplot captured exceptionally well. When employing the regression model of ‘glm’ we noticed that only one variable was significant at the highest level, ‘age’. This seems strange at first, but when we thought about it, it actually made a lot of sense since many of the factors that are known to contribute to the probability of having a stroke are more prevalent as one gets older, such as high blood pressure, hypertension, and even smoking (more common for older generations to partake in smoking than the younger ones of today). Therefore, we realized that it was risky to remove some, if not most, of these variables since they were essentially tied to the age variable - if we removed them we could seriously alter the outcome and fit of our model. So, we chose to keep them in even if the error was not as low as we may have wanted. This was apparent in our graphical representation of the model’s residuals against the fitted values, with there being a sharp contrast in some areas of the graph, particularly the right side. When considering the decision trees, however, we did have a more curated set of variables that we worked with and derived from our pruning and bagging methods. We chose to use female, age, hypertension, average glucose level, BMI, rural residence, and smoking status. Most of these clearly make sense apart from female and rural residences. However, ‘female’ may make more sense when we consider the amount of stress women can undertake on a daily basis as compared to men due to societal standards while a rural residence may just mean less access to primary care which may increase the risk of a stroke. Again, the split at age is very important as was earlier described, being more clearly put into two groups here, those being above and below the age of 55. The amount of error was higher than we had hoped, considering that the lecture models were close to being perfect. But when we stepped back and really thought about it, it made sense that our model would have a good amount of error since predicting a stroke can be incredibly difficult - if it wasn’t then the amount of the population that has experienced one would be much less than the 5% that our data set had. However, putting all of our findings together, we can see that there are clear variables that greatly increase one’s chance of having a stroke, and those are the things everyone should be paying attention to and monitoring in order to prevent a stroke event.
## Conclusions from the Analysis
1. **Age and hypertension identified as primary factors related to stroke occurrence**
2. **Additional factors include average glucose levels, BMI, and lifestyle choices**
3. **Acknowledged a relatively high error rate (15%) and suggested further research for improvement**

Based on the logistic regression conducted, we found that age and hypertension were primary factors that are related to the occurrence of a stroke. Similarly, through working on classification trees, we have reinforced the idea that there are multiple factors that can contribute to strokes. To reduce the stroke rate, it is essential to address additional several key factors including average glucose levels, BMI, and hypertension. Not to mention that implementing a healthy lifestyle such as having a balanced diet, exercising, quitting smoking, limiting alcohol consumption can also decrease the likelihood of a stroke as it directly affects these variables. It is also important to note that our analysis indicates that there is still a relatively high rate of error (15%). This suggests that there is a need for further research and exploration to enhance our understanding of stroke risk factors and improve prediction models. By continuing to investigate this field, we can strive to reduce the error rate and save lives by being able to implement more effective measures.
## Lessons Learned in the Project
1. **Standard linear regression not ideal for binary response; 'glm' introduced in week 6 was more suitable** 
2. **Challenges in communication and coordination within a group of four**
3. **Importance of sticking to schedules, constant reminders, and check-in meetings for group success**
   
In terms of the data set itself, we cover in our final presentation the variables that were most likely to cause a stroke, or at least increase the probability of having one. These variables shifted from including things like age, marital status, heart disease, and hypertension to variables such as age, BMI, and glucose. This shift may have been in part due to the presence of the ‘NA’ response in BMI (we assumed this was not removable and thus did not include BMI in our initial analysis, later realizing that it was easily fixed with the professor’s guidance (and command of ‘na omit’). Throughout the process of readying our data set and project, we learned that the standard linear regression modeling process works for a binary response variable, BUT is not ideal. We later learned the new method of ‘glm’ introduced in week 6, which much better modeled our data set to the ‘stroke’ variable. Additionally, this same problem was present when graphically modeling our data since the plots were difficult to read with only two different responses - lines would show up in seemingly random areas or data was simply clustered in one area, not necessarily providing any valuable data. We later learned, with the professor’s guidance, that things like box plots could still be made, but would only make sense when making our response variable the ‘x’ instead of the ‘y’, which was very out of the ordinary for us. Though this may not be representative of every single case and data set out there, we realized that our logistic regression and our decision tree had similar results and errors associated with them, which was interesting to us. This was in part due to the lectures exemplifying how much better decision trees could be while in our case we did not necessarily see a huge improvement/difference. Furthermore, the use of pruning and bagging methods in our own work helped highlight and reinforce concepts we had learned in the lecture, seeing as pruning the most we could was not necessarily the best choice. Outside of the project, we learned how hard it can be to effectively communicate and coordinate four different people since we had to take into account things like our sleeping schedules, class schedules, work, and free time. Thus, meeting up and actually dividing up work or seeing who needs to get what done seemed difficult. However, we did realize that when we met, it was important to stick to the schedule we had made in our planning phase whilst also having constant reminders in our group chat from each other to make sure work stayed on track. Check-in meetings (when possible) were thus essential to our group’s success. 

## Data Source and References
https://www.nhlbi.nih.gov/health/coronary-heart-disease

https://www.nhlbi.nih.gov/health/stroke/causes

https://www.nhlbi.nih.gov/health/high-blood-pressure

https://www.stroke.org.uk/what-is-stroke/are-you-at-risk-of-stroke#:~:text=As%20we%20get%20older%2C%20our,lead%20to%20an%20ischaemic%20stroke.

https://ochsnerlg.org/about-us/news/how-obesity-affects-stroke-risk#:~:text=Effects%20of%20Obesity&text=Repeated%20studies%20estimate%20that%20each,of%20stroke%20by%2050%20percent.
